{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: predictions and decisions for recommendation systems\n",
    "\n",
    "This notebook is modified from https://github.com/Heewon-Hailey/multi-armed-bandits-for-recommendation-systems/tree/main\n",
    "\n",
    "Predictions and recommendations based on the contextual information in system is an important and popular application. Multi-armed bandits (MABs) are a framework for sequential decision making under uncertainty. MABs solve problems in online advertising, information retrieval, and media recommendation. For instance, Yahoo! News decides what news items to recommend to users based on article content, user profile, and the historical engagement of the user with articles. Given decision making in this setting is sequential (what do we show next?) and feedback is only available for articles shown. MABs such as ɛ-Greedy and UCB show a perfect formulation. However, incorporating some element of user-article state requires contextual bandits: articles are arms; context per round incorporates information about both user and article (arm); and {0,1} -valued rewards represent clicks. Therefore the per round cumulative reward represents click-through-rate, which can maximise to drive user engagement and advertising revenue. \n",
    "\n",
    "## Recommendation systems for Bio-related areas:\n",
    "Recommender systems have a wide range of applications in biology-related areas. Here are a few examples:\n",
    "\n",
    "1. Drug discovery: Recommender systems can be used to predict the efficacy of drugs based on genomic and proteomic data.\n",
    "\n",
    "2. Clinical decision support: Recommender systems can assist doctors in making treatment decisions by recommending personalized therapies based on the patient's genetic data.\n",
    "\n",
    "3. Genomics research: Recommender systems can help researchers to identify relevant genes and genetic variants associated with specific diseases.\n",
    "\n",
    "4. Ecology and conservation biology: Recommender systems can be used to predict the distribution of species based on environmental data.\n",
    "\n",
    "5. Agricultural research: Recommender systems can be used to optimize crop yield by recommending optimal planting dates and crop varieties based on soil and weather data.\n",
    "\n",
    "6. Microbiology: Recommender systems can assist in the identification of microbial communities in environmental samples based on genomic data.\n",
    "\n",
    "7. Bioinformatics: Recommender systems can be used to suggest the most appropriate computational tools and methods for analyzing genomic and proteomic data.\n",
    "\n",
    "\n",
    "## Datasets\n",
    "\n",
    "In this notebook, we use the dataset `dataset.txt`, which contains 10,000 instances corrresponding to distinct site visits by users-events in the language of this part. Each instance comprises 102 space-delimited columns of integers:\n",
    " - Column 1: The arm played by a uniformly-random policy out of 10 arms (news articles)\n",
    " - Column 2: The reward received from the arm played|1 if the user clicked 0 otherwise; and\n",
    " - Columns 3-102: The 100-dim flattened context; 10 features per arm (incorporating the content of the article and its match with the visiting user), first the features for arm 1, then arm 2, etc. up to arm 10.\n",
    "\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "* Decisions (bandits): ɛ-greedy MAB; UCB MAB\n",
    "* Prediction and decision together: LinUCB contextual MAB including evaluation and hyperparameter tuning [1]\n",
    "For evaluation, off-policy evaluation [1-2] is implemented.\n",
    "\n",
    "\n",
    "## References \n",
    "\n",
    "[1] Lihong Li, Wei Chu, John Langford, Robert E. Schapire, ‘A Contextual-Bandit Approach to Personalized News Article Recommendation’, in Proceedings of the Nineteenth International Conference on World Wide Web (WWW’2010), Raleigh, NC, USA, 2010. \n",
    "https://arxiv.org/pdf/1003.0146.pdf\n",
    "\n",
    "[2] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. ‘Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms.’ In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining (WSDM’2011), pp. 297-306. ACM, 2011.\n",
    "https://arxiv.org/pdf/1003.5956.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit. These are the only imports permitted.\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some python background\n",
    "\n",
    "In Python, a parent class is a class that another class, known as a child class, inherits from. The child class can inherit attributes and methods from the parent class, and can also add its own attributes and methods. This allows for code reuse and can simplify the development process.\n",
    "\n",
    "An abstract method is a method declared in a parent class that does not have an implementation. Instead, the child class that inherits from the parent class must provide its own implementation of the abstract method. Abstract methods are useful for defining a common interface for a group of related classes, while allowing for each class to provide its own unique implementation of the method. In Python, an abstract method is defined using the `@abstractmethod` decorator, which indicates that the method is not meant to be called directly and must be implemented in a child class. Classes that contain abstract methods must also be marked as abstract using the `abc.ABC` or `abc.ABCMeta` metaclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB(ABC):\n",
    "    \"\"\"Base class for a contextual multi-armed bandit (MAB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "    \"\"\"\n",
    "    # initialise and raise input errors\n",
    "    def __init__(self, n_arms):\n",
    "        if not type(n_arms)==int:\n",
    "            raise TypeError(\"`n_arms` must be an integer\")\n",
    "        if not n_arms >= 0:\n",
    "            raise ValueError(\"`n_arms` must be non-negative\")\n",
    "        self.n_arms = n_arms\n",
    "        \n",
    "    @abstractmethod\n",
    "    # raise input errors\n",
    "    def play(self, context):\n",
    "        \"\"\"Play a round\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors presented to the MAB. The 0-th \n",
    "            axis indexes the arms, and the 1-st axis indexes the features.\n",
    "            Non-contextual bandits accept a context of None.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        arm : int\n",
    "            Integer index of the arm played this round. Should be in the set \n",
    "            {0, ..., n_arms - 1}.\n",
    "        \"\"\"\n",
    "        if not type(context) == np.ndarray:\n",
    "            raise TypeError(\"`context` must be numpy.ndarray\")\n",
    "        if not context.shape == (n_arms, n_dims):\n",
    "            raise TypeError(\"`context` must have shape (n_arms, n_dims)\")\n",
    "        self.context = context\n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    # raise input errors\n",
    "    def update(self, arm, reward, context):\n",
    "        \"\"\"Update the internal state of the MAB after a play\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arm : int\n",
    "            Integer index of the played arm in the set {0, ..., n_arms - 1}.\n",
    "        \n",
    "        reward : float\n",
    "            Reward received from the arm.\n",
    "        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors that was presented to the MAB. The \n",
    "            0-th axis indexes the arms, and the 1-st axis indexes the \n",
    "            features. Non-contextual bandits accept a context of None. \n",
    "        \"\"\"\n",
    "        if not (type(arm) == int or arm.dtype == 'int64'):\n",
    "            raise TypeError(\"`arm` must be int type\")\n",
    "        if not (arm >= 0 and arm <= (n_arms-1)):\n",
    "            raise ValueError(\"`arm` must be the the set {0, .., n_arms - 1}\")\n",
    "        if not (type(reward) == float or reward.dtype == 'float64'):\n",
    "            raise TypeError(\"`reward` must be float type\")\n",
    "        if not (context.shape == (n_arms, n_dims) and context.dtype == 'float64') :\n",
    "            raise TypeError(\"`context` must be float numpy in shape (n_events, n_arms, n_dims)\")\n",
    "        # get the values\n",
    "        self.arm = arm\n",
    "        self.reward = reward\n",
    "        self.context = context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global functions \n",
    "def break_tie(_range):\n",
    "    indices = np.argwhere(_range == np.max(_range))\n",
    "    index = np.random.randint(0,len(indices))\n",
    "\n",
    "    return indices[index][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset here\n",
    "\n",
    "data = np.loadtxt(\"dataset.txt\")\n",
    "arms, rewards, contexts = data[:,0], data[:,1], data[:,2:]\n",
    "arms = arms.astype(int)\n",
    "rewards = rewards.astype(float)\n",
    "contexts = contexts.astype(float)\n",
    "n_arms = len(np.unique(arms))\n",
    "n_events = len(contexts)\n",
    "n_dims = int(len(contexts[0])/n_arms)\n",
    "contexts = contexts.reshape(n_events, n_arms, n_dims)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy evaluation\n",
    "\n",
    "In the context of bandit problems, off-policy evaluation refers to the process of estimating the performance of a given policy using data collected under a different policy. In other words, we want to estimate how well a new policy would perform in the future based on historical data collected by a different policy. This is important because in many real-world scenarios, it may not be practical or ethical to experiment with a new policy on live users. Off-policy evaluation can be done using techniques such as importance sampling or weighted importance sampling. These methods re-weight the historical data to reflect the differences between the behavior policy and the target policy, allowing us to estimate the expected rewards that the target policy would have obtained if it had been used to collect the data.\n",
    "Below we have a simplified version of offline evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offlineEvaluate(mab, arms, rewards, contexts, n_rounds=None):\n",
    "    \"\"\"Offline evaluation of a multi-armed bandit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mab : instance of MAB\n",
    "        MAB to evaluate.\n",
    "    \n",
    "    arms : integer numpy.ndarray, shape (n_events,) \n",
    "        Array containing the history of pulled arms, represented as integer \n",
    "        indices in the set {0, ..., mab.n_arms}\n",
    "    \n",
    "    rewards : float numpy.ndarray, shape (n_events,)\n",
    "        Array containing the history of rewards.\n",
    "    \n",
    "    contexts : float numpy.ndarray, shape (n_events, n_arms, n_dims)\n",
    "        Array containing the history of contexts presented to the arms. \n",
    "        The 0-th axis indexes the events in the history, the 1-st axis \n",
    "        indexes the arms and the 2-nd axis indexed the features.\n",
    "        \n",
    "    n_rounds : int, default=None\n",
    "        Number of matching events to evaluate the MAB on. If None, \n",
    "        continue evaluating until the historical events are exhausted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : float numpy.ndarray\n",
    "        Rewards for the matching events.\n",
    "    \"\"\"\n",
    "    # initialise values and raise input errors\n",
    "    if not (arms.shape == (n_events,) and arms.dtype == 'int64')  :\n",
    "        raise TypeError(\"`arms` must be integer numpy in shape (n_events,)\")\n",
    "    if not rewards.shape == (n_events,) and rewards.dtype == 'float64' :\n",
    "        raise TypeError(\"`rewards` must be float numpy in shape (n_events,)\")\n",
    "    if not contexts.shape == (n_events,n_arms, n_dims) and rewards.dtype == 'float64' :\n",
    "        raise TypeError(\"`contexts` must be float numpy in shape (n_events, n_arms, n_dims)\")\n",
    "    if n_rounds == None:        # set n_rounds to infinite number to run until all data exhausted\n",
    "        n_rounds = np.inf\n",
    "    elif not type(n_rounds) == int:\n",
    "        raise TypeError(\"`n_rounds` must be integer or default 'None'\")\n",
    "\n",
    "    n_round = 0     # count the current round ; 0 indicates the first round\n",
    "    R = []          # save the total payoff\n",
    "    H = []          # save used historical events\n",
    "    \n",
    "    for i in range(n_events):\n",
    "        if n_round == n_rounds:\n",
    "            break\n",
    "        arm = mab.play(contexts[i])\n",
    "        if arm == arms[i]:                 # if historical data equals to chosen arm\n",
    "            R.append(rewards[i])           # append the new rewards\n",
    "            H.append([arms[i], rewards[i], contexts[i]])      # append the used events\n",
    "            mab.update(arms[i], rewards[i], contexts[i])      # update the information\n",
    "            n_round += 1\n",
    "\n",
    "    # return rewards per play\n",
    "    out = np.array(R)\n",
    "        \n",
    "    return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ε-greedy MAB\n",
    "\n",
    "The epsilon-greedy algorithm is a popular approach in reinforcement learning and multi-armed bandit problems. It balances exploration and exploitation by selecting actions based on a trade-off between choosing the current best action (greedy) and exploring other actions (random). \n",
    "\n",
    "It works by assigning a small probability (epsilon) for random exploration, during which a random action is chosen, and a higher probability (1 - epsilon) for exploiting the current best action based on previous knowledge or learned values. This algorithm allows for the discovery of new potentially rewarding actions while still favoring actions with higher expected rewards based on the available information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedy(MAB):\n",
    "    \"\"\"Epsilon-Greedy multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms\n",
    "\n",
    "    epsilon : float\n",
    "        Explore probability. Must be in the interval [0, 1].\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    # initialise values and raise input errors\n",
    "    def __init__(self, n_arms, epsilon, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        if not (epsilon >= 0 and epsilon <= 1):\n",
    "            raise ValueError(\"`epsilon` must be a number in [0,1]\")\n",
    "        if not type(epsilon) == float:\n",
    "            raise TypeError(\"`epsilon` must be float\")\n",
    "        if not type(Q0) == float:\n",
    "            raise TypeError(\"`Q0` must be a float number or default value 'np.inf'\")\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.q = np.full(n_arms, Q0)      # initialise q values\n",
    "        self.rewards = np.zeros(n_arms)     # keep the total rewards per arm\n",
    "        self.clicks = np.zeros(n_arms)      # count the pulled rounds per arm\n",
    "    \n",
    "    # select a random arm to explore or a arm with best rewards to exploit, then return the arm \n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        \n",
    "        # ------------------------------- #\n",
    "        #       TODO: recommend arm       #\n",
    "        arm = None\n",
    "        #---------------------------------#\n",
    "        return arm\n",
    "    \n",
    "    # update values\n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        self.clicks[arm] += 1\n",
    "        self.rewards[arm] += self.reward\n",
    "        self.q[arm] = self.rewards[arm] / self.clicks[arm]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = EpsGreedy(10, 0.05) \n",
    "results_EpsGreedy = offlineEvaluate(mab, arms, rewards, contexts, 800)\n",
    "print('EpsGreedy average reward', np.mean(results_EpsGreedy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upper Confidence Bound (UCB)\n",
    "\n",
    "The Upper Confidence Bound (UCB) algorithm is a strategy commonly used in multi-armed bandit problems to balance exploration and exploitation. It assigns each arm (action) a score based on its potential for high reward, taking into account both the average reward obtained from that arm and the uncertainty or variance associated with that estimate. The UCB algorithm selects the arm with the highest score at each step, prioritizing arms that have shown promise but have not been extensively explored. As more data is gathered, the algorithm becomes more confident in its estimates and gradually focuses on exploiting the arms with the highest potential for reward. The UCB algorithm offers a principled approach to optimize the trade-off between exploration and exploitation in decision-making tasks.\n",
    "\n",
    "$$U C B_t(a)=\\hat{\\mu}_t(a)+\\sqrt{\\frac{\\rho \\log t}{T_t(a)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(MAB):\n",
    "    \"\"\"Upper Confidence Bound (UCB) multi-armed bandit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    rho : float\n",
    "        Positive real explore-exploit parameter.\n",
    "\n",
    "    Q0 : float, default=np.inf\n",
    "        Initial value for the arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, rho, Q0=np.inf):\n",
    "        super().__init__(n_arms)\n",
    "        if not rho > 0:\n",
    "            raise ValueError(\"`rho` must be positive\")\n",
    "        if not (type(rho) == float and np.isreal(rho)):\n",
    "            raise TypeError(\"`rho` must be real float\")\n",
    "        if not type(Q0) == float :\n",
    "            raise TypeError(\"`Q0` must be a float number or default value 'np.inf'\")\n",
    "            \n",
    "        self.rho = rho\n",
    "        self.q = np.full(n_arms, Q0)\n",
    "        self.rewards = np.zeros(n_arms)  \n",
    "        self.avg_rewards = np.zeros(n_arms)\n",
    "        self.clicks = np.zeros(n_arms)\n",
    "        self.round = 0        # to count the number of round played\n",
    "    \n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        self.round += 1\n",
    "        # ------------------------------- #\n",
    "        #       TODO: calculate self.q    #\n",
    "        self.q = None\n",
    "        #---------------------------------#\n",
    "        \n",
    "        arm = break_tie(self.q)\n",
    "        \n",
    "        return int(arm)\n",
    "        \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        self.clicks[arm] += 1\n",
    "        self.rewards[arm] += reward\n",
    "        self.avg_rewards[arm] = self.rewards[arm]/ self.clicks[arm]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soluation \n",
    "# self.q = np.where(self.clicks != 0, self.avg_rewards + np.sqrt(self.rho * np.log10(self.round) / self.clicks), self.q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning control\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = UCB(10, 1.0)\n",
    "results_UCB = offlineEvaluate(mab, arms, rewards, contexts,)\n",
    "print('UCB average reward', np.mean(results_UCB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (Contextual) LinUCB \n",
    "\n",
    "Now we put predictions and decision making together:\n",
    "\n",
    "The LinUCB contextual algorithm is a variant of the Upper Confidence Bound (UCB) algorithm designed for contextual bandit problems. In contextual bandits, the algorithm receives additional contextual information or features along with each arm (action). The LinUCB algorithm models the relationship between the contextual features and rewards using linear regression. It maintains a set of linear models for each arm, estimating the expected reward based on the observed contextual information. The algorithm combines the estimated rewards with confidence bounds, computed using the uncertainty in the regression estimates, to make decisions about which arm to select. By incorporating contextual information and modeling the reward function through linear regression, the LinUCB algorithm aims to adaptively learn the optimal policy in contextual bandit settings, effectively balancing exploration and exploitation.\n",
    "\n",
    "$$a_t \\stackrel{\\text { def }}{=} \\arg \\max _{a \\in \\mathcal{A}_t}\\left(\\mathbf{x}_{t, a}^{\\top} \\hat{\\boldsymbol{\\theta}}_a+\\alpha \\sqrt{\\mathbf{x}_{t, a}^{\\top} \\mathbf{A}_a^{-1} \\mathbf{x}_{t, a}}\\right)$$\n",
    "where $\\mathbf{A}_a \\stackrel{\\text { def }}{=} \\mathbf{D}_a^{\\top} \\mathbf{D}_a+\\mathbf{I}_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB(MAB):\n",
    "    \"\"\"Contextual multi-armed bandit (LinUCB)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    alpha : float\n",
    "        Positive real explore-exploit parameter.\n",
    "    \"\"\"\n",
    "    # initialise values and raise input errors\n",
    "    def __init__(self, n_arms, n_dims, alpha):\n",
    "        if not (type(n_dims) == int or n_dims.dtype == 'int64'):\n",
    "            raise TypeError(\"`n_dims` must be integer type\")\n",
    "        if not (type(alpha) == float or alpha.dtype == 'float64'):\n",
    "            raise TypeError(\"`alpha` must be float\")\n",
    "        if not (alpha > 0.0 and np.isreal(alpha)):\n",
    "            raise ValueError(\"`alpha` must be positive real\")\n",
    "        \n",
    "        super().__init__(n_arms) \n",
    "        self.n_dims = n_dims\n",
    "        self.alpha = alpha\n",
    "        self.post_dist = np.zeros(n_dims)\n",
    "        '''initialise keys and values; key is arm, A for covariance, inv_A for inverse of A, \n",
    "                                        b for reward, theta for coefficient vector''' \n",
    "        self.A = np.array(np.identity(n_dims))\n",
    "        self.inv_A = [np.linalg.inv(self.A)]*10\n",
    "        self.A  = [self.A]*10\n",
    "\n",
    "        self.b = [np.zeros(n_dims)]*10\n",
    "        self.theta = [(np.linalg.inv(np.identity(n_dims)) @  np.zeros(n_dims))]*10\n",
    "         \n",
    "    # return the best arm\n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        # calculate posterior distribution of the coefficient vector \n",
    "        for arm in range(self.n_arms):\n",
    "            inv_A = self.inv_A[arm]\n",
    "            theta = self.theta[arm]\n",
    "\n",
    "            \n",
    "            # ------------------------------- #\n",
    "            # TODO: calculate posterior distribution of the coefficient vector     \n",
    "            self.post_dist[arm] = None\n",
    "            #---------------------------------#\n",
    "            \n",
    "            \n",
    "        arm = break_tie(self.post_dist)\n",
    "        return int(arm)    \n",
    "    \n",
    "    # update dictionary\n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        reshaped_context = context[arm].reshape(-1,1)   # reshape to the right form\n",
    "        self.A[arm] = self.A[arm] + reshaped_context @ reshaped_context.T\n",
    "        self.inv_A[arm] = np.linalg.inv(self.A[arm])\n",
    "        self.b[arm] = self.b[arm] + reward * context[arm]\n",
    "        self.theta[arm] = self.inv_A[arm] @ self.b[arm]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# self.post_dist[arm] = theta @ context[arm] + self.alpha * np.sqrt(context[arm].T @ inv_A @ context[arm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = LinUCB(10, 10, 1.0)\n",
    "results_LinUCB = offlineEvaluate(mab, arms, rewards, contexts,800)\n",
    "print('LinUCB average reward', np.mean(results_LinUCB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative reward per round \n",
    "eg_results, ucb_results, linucb_results, tbs_results, round_list = [], [], [], [], []  # create lists\n",
    "n_rounds = 800                                       # the total number of rounds \n",
    "eg_sum, ucb_sum,linucb_sum, tbs_sum = 0, 0, 0, 0     # set the initial reward sum\n",
    "\n",
    "# get the 800 results from the previous run per each algorithm \n",
    "for n_round in range(1, n_rounds + 1):              # start from 1 to avoid zero devision error\n",
    "    eg_sum += results_EpsGreedy[n_round-1]\n",
    "    ucb_sum += results_UCB[n_round-1]\n",
    "    linucb_sum += results_LinUCB[n_round-1] \n",
    "    eg_results.append(eg_sum/n_round)\n",
    "    ucb_results.append(ucb_sum/n_round)\n",
    "    linucb_results.append(linucb_sum/n_round)\n",
    "    round_list.append(n_round)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(round_list,eg_results, label = \"EpsGreedy\")\n",
    "plt.plot(round_list,ucb_results, label = \"UCB\")\n",
    "plt.plot(round_list,linucb_results, label = \"LinUCB\")\n",
    "\n",
    "plt.ylabel('Cumulative Reward Per-Round')\n",
    "plt.xlabel('Rounds')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
